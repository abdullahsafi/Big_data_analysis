{"cells":[{"cell_type":"markdown","source":["# Task 1 RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68a9b90e-2c3e-4124-a13c-3081998c0d69"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom sparkmeasure import StageMetrics\nfrom pyspark.sql import functions as F\nfrom pyspark import SparkConf, SparkContext\n\nfrom operator import add\n\nspark = SparkSession.builder.appName(\"task1\").getOrCreate()\n\nstagemetrics = StageMetrics(spark)\n\ndbfs_fileStore_prefix = \"/FileStore/tables\"\nprefix = \"ontimeperformance\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a79115ef-dd6f-4d58-8f86-b9911dd742cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# https://stackoverflow.com/questions/55453101/pyspark-error-analysisexception-cannot-resolve-column-name\ndef clean_column_names(df):\n  tempList = [] #Edit01\n  for col in df.columns:\n      new_name = col.strip()\n      new_name = \"\".join(new_name.split())\n      new_name = new_name.replace('.','') \n      tempList.append(new_name) \n\n  return df.toDF(*tempList) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d4e0e82-224e-4e39-bbdb-7e11cb99cf8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import re\ndef task_1_rdd(spark_session, flights_path, aircrafts_path):\n  flights_rdd = spark.sparkContext.textFile(flights_path).map(lambda x: x.split(\",\")) \n  aircrafts_rdd = spark.sparkContext.textFile(aircrafts_path).map(lambda x: x.split(\",\")) \n\n  # remove header and any row with null\n  header = aircrafts_rdd.first()\n  aircrafts_rdd = aircrafts_rdd.filter(lambda x: len(x) == len(header))\n  aircrafts_rdd = aircrafts_rdd.filter(lambda x: x != header)\n  # remove entries which are not cessna\n  aircrafts_rdd = aircrafts_rdd.filter(lambda x: x[2] == \"CESSNA\")\n  # filter out cancelled flights\n  flights_rdd = flights_rdd.filter(lambda x : x[5] != \"\")\n  # get list of (tail_num, model) from aircraft\n  aircrafts_rdd_small = aircrafts_rdd.map(lambda x: (x[0], x[4]))\n  # turn aircraft_rdd_small's model into just 3 digits\n  aircrafts_rdd_small = aircrafts_rdd_small.map(lambda x: (x[0], re.findall(r'\\d{3}', x[1])[0]))\n  # list of (tail_num, carrier_code) from flights\n  flights_rdd_small = flights_rdd.map(lambda x: (x[5], x[0]))\n  \n  joined_rdd = aircrafts_rdd_small.join(flights_rdd_small)\n  final = joined_rdd.map(lambda x: (x[1][0], 1))\n  \n  counts = final.reduceByKey(lambda a, b: a + b).takeOrdered(3, key=lambda x: -x[1])\n  for i in range(len(counts)):\n     print(\"Cessna {} \\t {}\".format((re.findall(r'\\d{3}', counts[i][0])[0]), counts[i][1]))\n  \ntask_1_rdd(spark, f\"{dbfs_fileStore_prefix}/{prefix}_flights_small.csv\", f\"{dbfs_fileStore_prefix}/{prefix}_aircrafts.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b02a8123-be25-4545-8327-4c6996787f1a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Cessna 172 \t 57\nCessna 210 \t 48\nCessna 421 \t 47\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Cessna 172 \t 57\nCessna 210 \t 48\nCessna 421 \t 47\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task-1-rdd","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2766873644733411}},"nbformat":4,"nbformat_minor":0}
